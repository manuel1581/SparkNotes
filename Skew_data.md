# Data Skew

This happens when we have partitions with more data to process than others. We want an equally distributed amount of data in every
partition in order to parallelize our workloads.


References: 
https://www.davidmcginnis.net/post/spark-job-optimization-dealing-with-data-skew
https://kontext.tech/column/spark/296/data-partitioning-in-spark-pyspark-in-depth-walkthrough
https://towardsdatascience.com/the-art-of-joining-in-spark-dcbd33d693c
https://unraveldata.com/common-failures-slowdowns-part-ii/
https://stackoverflow.com/questions/40373577/skewed-dataset-join-in-spark
https://datarus.wordpress.com/2015/05/04/fighting-the-skew-in-spark/
